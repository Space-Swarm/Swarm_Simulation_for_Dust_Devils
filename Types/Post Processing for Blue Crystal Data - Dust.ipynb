{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swarm Simulator for Outer Space\n",
    "\n",
    "This swarm simulator will explore different swarm algorithms for an application in outer space. This approach will divide a plot into grid squares, which can then be analysed to determine and control the behaviour of each individual agent in the swarm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sympy as sym\n",
    "import plotly as py\n",
    "import plotly.tools as tls\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import Image\n",
    "\n",
    "import dash\n",
    "from dash.dependencies import Output, Input\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import math\n",
    "\n",
    "from scipy import special\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from collections import deque\n",
    "from scipy.spatial.distance import cdist, pdist, euclidean\n",
    "from sympy import sin, cos, symbols, lambdify\n",
    "\n",
    "from robot import Robot\n",
    "from dust_devil import DustDevil\n",
    "from live_functions import initialise,random_position, grid_center,positions,bounce,magnitude,unit,division_check,physics_walk,dust_check,update_timestep,cluster_function,G_transition,dist,random_walk,dust,trajectory_dust,update_dust,load_positions,pre_initialise\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import Processing_Functions_BP\n",
    "\n",
    "\n",
    "import dash\n",
    "from dash.dependencies import Output, Input\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "app = dash.Dash(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotly offline mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_load(path):\n",
    "    store_robots = np.load(path + \"Robots.npy\")\n",
    "    with open(path + \"dust.txt\", \"r\") as f:\n",
    "        store_dust = json.load(f)\n",
    "\n",
    "\n",
    "    constants = pd.read_excel(path + \"Constants.xlsx\", index_col=0)\n",
    "    min_neighbours = np.load(path + 'Minimum Distance to Neighbours.npy')\n",
    "    cluster_average = np.load(path + 'Cluster Average.npy')\n",
    "    total_collision = np.load(path + 'Measurement Events Count.npy')\n",
    "    total_detection = np.load(path + 'Number of Dust Devils Detected.npy')\n",
    "    total_dust = np.load(path + 'Number of Dust Devils Generated.npy')\n",
    "    #setting the start paths for the graphs and the tables\n",
    "    return constants,min_neighbours,cluster_average,total_collision,total_detection,total_dust\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190.           9.        ]\n",
      " [200.           9.        ]\n",
      " [200.           9.        ]\n",
      " [205.           9.        ]\n",
      " [205.           9.        ]\n",
      " [205.55555556   9.        ]\n",
      " [210.           9.        ]\n",
      " [211.11111111   9.        ]\n",
      " [216.66666667   9.        ]\n",
      " [222.22222222   9.        ]\n",
      " [227.77777778   9.        ]\n",
      " [233.33333333   9.        ]\n",
      " [238.88888889   9.        ]\n",
      " [244.44444444   9.        ]\n",
      " [250.           9.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "word = \"Robots\"\n",
    "word_run = \"Run_\"\n",
    "array = [10]#,20,30,40,50,60,70,80,90,100]\n",
    "runs = 1\n",
    "directory = \"../Experiments/Area Coverage 2.2. Testing Optimizer/\"\n",
    "image = directory + \"Images/\"\n",
    "\n",
    "final = []\n",
    "\n",
    "time = 5000\n",
    "for i in array:\n",
    "    outer_path = directory + str(i) + \" \" + word + \"/\"\n",
    "    for j in range(runs):\n",
    "\n",
    "        path = outer_path + word_run + str(j) + \"/\"\n",
    "        print(np.load(path+\"Area_Coverage_Results.npy\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2\n",
      "10 Robots \n",
      "10 Robots \n",
      "10 Robots \n",
      "[[[10, 27]]]\n",
      "[  0   0   0 ... 420 420 420]\n",
      "420\n",
      "20\n",
      "2\n",
      "20 Robots \n",
      "20 Robots \n",
      "20 Robots \n",
      "[[[10, 27]], [[20, 46]]]\n",
      "[  0   0   0 ... 420 420 420]\n",
      "420\n",
      "30\n",
      "2\n",
      "30 Robots \n",
      "30 Robots \n",
      "30 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]]]\n",
      "[  0   0   0 ... 421 421 421]\n",
      "421\n",
      "40\n",
      "2\n",
      "40 Robots \n",
      "40 Robots \n",
      "40 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]]]\n",
      "[  0   0   0 ... 420 421 421]\n",
      "421\n",
      "50\n",
      "2\n",
      "50 Robots \n",
      "50 Robots \n",
      "50 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]], [[50, 94]]]\n",
      "[  0   0   0 ... 422 422 422]\n",
      "422\n",
      "60\n",
      "2\n",
      "60 Robots \n",
      "60 Robots \n",
      "60 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]], [[50, 94]], [[60, 108]]]\n",
      "[  0   0   0 ... 421 421 421]\n",
      "421\n",
      "70\n",
      "2\n",
      "70 Robots \n",
      "70 Robots \n",
      "70 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]], [[50, 94]], [[60, 108]], [[70, 118]]]\n",
      "[  0   0   0 ... 422 422 422]\n",
      "422\n",
      "80\n",
      "2\n",
      "80 Robots \n",
      "80 Robots \n",
      "80 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]], [[50, 94]], [[60, 108]], [[70, 118]], [[80, 129]]]\n",
      "[  0   0   0 ... 422 422 422]\n",
      "422\n",
      "90\n",
      "2\n",
      "90 Robots \n",
      "90 Robots \n",
      "90 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]], [[50, 94]], [[60, 108]], [[70, 118]], [[80, 129]], [[90, 124]]]\n",
      "[  0   0   0 ... 421 421 421]\n",
      "421\n",
      "100\n",
      "2\n",
      "100 Robots \n",
      "100 Robots \n",
      "100 Robots \n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]], [[50, 94]], [[60, 108]], [[70, 118]], [[80, 129]], [[90, 124]], [[100, 149]]]\n",
      "[  0   0   0 ... 423 423 423]\n",
      "423\n",
      "[[[10, 27]], [[20, 46]], [[30, 60]], [[40, 91]], [[50, 94]], [[60, 108]], [[70, 118]], [[80, 129]], [[90, 124]], [[100, 149]]]\n"
     ]
    }
   ],
   "source": [
    "word = \"Robots\"\n",
    "word_run = \"Run_\"\n",
    "array = [10,20,30,40,50,60,70,80,90,100]\n",
    "runs = 10\n",
    "directory = \"Blue_Pebble/Fixed/\"\n",
    "image = directory + \"Images/\"\n",
    "time = 86400\n",
    "final = []\n",
    "frequency = 1\n",
    "for i in array:\n",
    "    outer_path = directory + str(i) + \" \" + word + \"/\"\n",
    "    for j in range(runs):\n",
    "\n",
    "        path = outer_path + word_run + str(j) + \"/\"\n",
    "        code = str(i) + \" \" + word + \" \"\n",
    "        store_robots = np.load(path + \"Robots.npy\")\n",
    "        with open(path + \"dust.txt\", \"r\") as f:\n",
    "            store_dust = json.load(f)\n",
    "        print(len(list(store_robots)))\n",
    "        constants = pd.read_excel(path + \"Constants.xlsx\", index_col=0)\n",
    "        min_neighbours = np.load(path + 'Minimum Distance to Neighbours.npy')\n",
    "        cluster_average = np.load(path + 'Cluster Average.npy')\n",
    "        total_collision = np.load(path + 'Measurement Events Count.npy')\n",
    "        total_detection = np.load(path + 'Number of Dust Devils Detected.npy')\n",
    "        total_dust = np.load(path + 'Number of Dust Devils Generated.npy')\n",
    "        #setting the start paths for the graphs and the tables\n",
    "       #setting the start paths for the graphs and the tables\n",
    "        graph_start_path = image + code + \"- Graph_Beginning.png\"\n",
    "        table_start_path = image +code + \"- Table_Beginning.png\"\n",
    "\n",
    "        #using the processing functions to create plotly graphs and tables for the figures in the first timestep\n",
    "        graph_start = Processing_Functions_Tracking.graph_figure(store_robots,0,frequency,code)\n",
    "        graph_start.write_image(graph_start_path)\n",
    "        table_start = Processing_Functions_Tracking.table_figure_area(store_robots,0,frequency,constants,min_neighbours,cluster_average)\n",
    "        table_start.write_image(table_start_path)\n",
    "\n",
    "        #combining the tables and the graphs using pillow\n",
    "        Processing_Functions_Tracking.combine(graph_start_path,table_start_path)\n",
    "        #setting the end paths for the graphs and the tables\n",
    "        graph_end_path = image + code + \"- Graph_End.png\"\n",
    "        table_end_path =image + code + \"- Table_End.png\"\n",
    "\n",
    "        #using the processing functions to create plotly graphs and tables for the figures in the last timestep\n",
    "        graph_end = Processing_Functions_Tracking.graph_figure(store_robots,time-1,frequency,code)\n",
    "        graph_end.write_image(image + code + \"- Graph_End.png\")\n",
    "        table_end = Processing_Functions_Tracking.table_figure_area(store_robots,time-1,frequency,constants,min_neighbours,cluster_average)\n",
    "        table_end.write_image(image + code + \"- Table_End.png\")\n",
    "\n",
    "        #using the processing functions to create plotly graphs and tables for the figures in the last timestep\n",
    "        graph_end_types = Processing_Functions_Tracking.graph_types(x_0,y_0,x_1,y_1,500,10,\"Deployed Swarm Formation \", \" <b>Timestep = \" + str(time) + \" s<br>R = \" + str(round(R,2)) + \"</b> <br> \")\n",
    "        graph_end_types.write_image(image + code + \"- Graph_End_Types.png\")\n",
    "\n",
    "\n",
    "        #using the processing functions to create plotly graphs and tables for the figures in the last timestep\n",
    "        graph_end_area_coverage = Processing_Functions_Tracking.graph_area_coverage(x_0,y_0,x_1,y_1,500,10, \"Area Coverage Over a Grid for a Deployed Swarm Formation\",\" <b>Timestep = \" + str(time) + \" s<br>R = \" + str(round(R,2)) + \"<br>Area Coverage = \" + str(current_grid_metric) + \"%</b><br> \")\n",
    "        graph_end_area_coverage.write_image(image + code + \"- Graph_End_Area_Coverage.png\")\n",
    "        #table_end_types = Processing_Functions_Tracking.table_figure(store_robots,time-1,frequency,constants,min_neighbours,cluster_average,total_collision,total_detection,total_dust)\n",
    "        #table_end.write_image(image + \"/\" + code + \"- Table_Types_End.png\")\n",
    "\n",
    "        #combining the tables and the graphs using pillow\n",
    "        Processing_Functions_Tracking.combine(graph_end_path,table_end_path)\n",
    "\n",
    "        #plotting performance of the average of minimum neighbouring distance metric\n",
    "        performance = Processing_Functions_Tracking.performance_graph(min_neighbours,np.linspace(0,len(min_neighbours),len(min_neighbours)*frequency,endpoint = False),frequency,code,\"Time (s)\",\"Minimum Average Neighbour Distance (m)\")\n",
    "        performance.write_image(image + code + \"- Minimum Neighbour Average.png\")\n",
    "\n",
    "        #plotting performance of the dust devil measurement metric\n",
    "        performance_intercept = Processing_Functions_Tracking.performance_graph(total_collision,np.linspace(0,len(total_collision),len(total_collision)*frequency,endpoint = False),frequency,code,\"Time (s)\",\"Count of Measurement Events\")\n",
    "        performance_intercept.write_image(image + \"/\" + code + \"- Intercept Performance.png\")\n",
    "\n",
    "        #plotting performance of the cluster average of the swarm\n",
    "        cluster = Processing_Functions_Tracking.performance_graph(cluster_average,np.linspace(0,len(cluster_average),len(cluster_average)*frequency,endpoint = False),frequency,code,\"Time (s)\",\"Average Cluster Size\")\n",
    "        cluster.write_image(image + code +  \"- Average Cluster Size.png\")\n",
    "\n",
    "        final_np = np.load(path+'Final Dust Devil Count.npy')\n",
    "        final.append([list(final_np[-1])])\n",
    "        print(final)\n",
    "        print(total_dust)\n",
    "        print((total_dust[-1]))\n",
    "    print(final)\n",
    "    np.save(directory + \"Performance vs Robot Number.npy\",final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metric_np = np.array(final)\n",
    "performance_metric = performance_metric_np[:,0]\n",
    "print(performance_metric[:,1])\n",
    "performance_overall = Processing_Functions_BP.performance_graph(performance_metric[:,1],performance_metric[:,0],1,\"\",\"Number of Robots\",\"Number of Dust Devils Detected\")\n",
    "performance_overall.write_image(directory + \"Detection Performance vs Robot Number.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = \"Blue_Pebble/Robot Array/10 Robots%20/\"\n",
    "directory2 = \"Blue_Pebble/Robot Array/30 Robots%20/\"\n",
    "with open(directory1 + \"dust.txt\", \"r\") as f:\n",
    "        store_dust1 = json.load(f)\n",
    "with open(directory2 + \"dust.txt\", \"r\") as f:\n",
    "        store_dust2 = json.load(f)\n",
    "print(store_dust1[10000])\n",
    "print(store_dust2[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-156.26497956057256,-156.5109418975476 73.9590318896967"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
